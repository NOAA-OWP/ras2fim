import os
import argparse
import numpy as np
from datetime import datetime
import pandas as pd
import shutil
import geopandas as gpd
import rasterio
import errno
from rasterio.features import shapes
from concurrent.futures import ProcessPoolExecutor, as_completed, wait
VIZ_PROJECTION ='PROJCS["WGS_1984_Web_Mercator_Auxiliary_Sphere",GEOGCS["GCS_WGS_1984",DATUM["D_WGS_1984",SPHEROID["WGS_1984",6378137.0,298.257223563]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]],PROJECTION["Mercator_Auxiliary_Sphere"],PARAMETER["False_Easting",0.0],PARAMETER["False_Northing",0.0],PARAMETER["Central_Meridian",0.0],PARAMETER["Standard_Parallel_1",0.0],PARAMETER["Auxiliary_Sphere_Type",0.0],UNIT["Meter",1.0]]'
from shapely.geometry.polygon import Polygon
from shapely.geometry.multipolygon import MultiPolygon
from shapely.geometry import MultiPolygon, Polygon, LineString, Point
#from shapely.validation import make_valid



def produce_geocurves(feature_id, huc, rating_curve, depth_grid_list, version, output_folder):
    """
    For a single feature_id, the function produces a version of a RAS2FIM rating curve 
    which includes the geometry of the extent for each stage/flow.
    
    Args:
        feature_id (str): National Water Model feature_id.
        huc (str): Derived from the directory names of the 05_hec_ras outputs which are organized by HUC12.
        rating_curve(str): The path to the feature_id-specific rating curve generated by RAS2FIM.
        depth_grid_list (list): A list of paths to the depth grids generated by RAS2FIM for the feature_id.
        version (str): The version number.
        output_folder (str): Path to output folder where geo version of rating curve will be written.
        
    """

    # Read rating curve for feature_id
    rating_curve_df = pd.read_csv(rating_curve)

    # Loop through depth grids and store up geometries to collate into a single rating curve.
    iteration = 0
    for depth_grid in depth_grid_list:
        print(depth_grid)

        # Interpolate flow from given stage.
        stage_mm = float(os.path.split(depth_grid)[1].split('-')[1].strip('.tif'))
        stage_m = stage_mm/1000.0
        print(stage_m)
        # interpolated_flow_cfs = np.interp(stage_ft,rating_curve_df.loc[:,'AvgDepth(ft)'],rating_curve_df.loc[:,'Flow(cfs)'])
        with rasterio.open(depth_grid) as src:
            # Open inundation_raster using rasterio.
            image = src.read(1)
#            mask = image > 0
#            print("Producing merged polygon...")
        
#            print(src.crs)
            # Use numpy.where operation to reclassify depth_array on the condition that the pixel values are > 0.
            reclass_inundation_array = np.where((image>0) & (image != src.nodata), 1, 0).astype('uint8')

#            results = ({'properties': {'extent': 1}, 'geometry': s} for i, (s, v) in enumerate(shapes(image, mask=mask,transform=src.transform)))

            # Aggregate shapes
            results = ({'properties': {'extent': 1}, 'geometry': s} for i, (s, v) in enumerate(shapes(reclass_inundation_array, mask=reclass_inundation_array>0,transform=src.transform)))
    
            # Convert list of shapes to polygon, then dissolve
            extent_poly = gpd.GeoDataFrame.from_features(list(results), crs='EPSG:5070')
            try:
                extent_poly_diss = extent_poly.dissolve(by='extent')
                extent_poly_diss["geometry"] = [MultiPolygon([feature]) if type(feature) == Polygon else feature for feature in extent_poly_diss["geometry"]]
            except AttributeError:
                print(huc)
                print(feature_id)
                continue
#            
            # Write polygon
            inundation_polygon = os.path.join(output_folder, feature_id + '_' + huc + '_' + str(stage_mm) + '_mm '+ '.gpkg')
            # -- Add more attributes -- #
            extent_poly_diss['version'] = version
            extent_poly_diss['feature_id'] = feature_id
            extent_poly_diss['stage_mm_join'] = stage_mm
            extent_poly_diss['path'] = inundation_polygon
            
#            extent_poly_diss['geometry'] = extent_poly_diss.apply(lambda row: make_valid(row.geometry), axis=1)
            try:
                extent_poly_diss['valid'] = extent_poly_diss.is_valid
            except:
                continue
#            extent_poly_diss = extent_poly_diss[extent_poly_diss['valid'] == True]
            
#            wkt = extent_poly_diss.geom.apply(lambda x: x.wkt)
            
            extent_poly_diss.to_file(inundation_polygon, driver='GPKG')
            if iteration < 1:  # Initialize the rolling huc_rating_curve_geo
                feature_id_rating_curve_geo = pd.merge(rating_curve_df, extent_poly_diss, left_on='stage_mm', right_on='stage_mm_join', how='right')
            else:
                rating_curve_geo_df = pd.merge(rating_curve_df, extent_poly_diss, left_on='stage_mm', right_on='stage_mm_join', how='right')
                feature_id_rating_curve_geo = pd.concat([feature_id_rating_curve_geo, rating_curve_geo_df])
            iteration += 1 
    

#    print("Making valid...")
#    feature_id_rating_curve_geo['valid'] = feature_id_rating_curve_geo.is_valid  # Add geometry validity column
#    feature_id_rating_curve_geo = feature_id_rating_curve_geo[feature_id_rating_curve_geo['valid'] == True]
#    
    print("Writing to CSV...")
    feature_id_rating_curve_geo.to_csv(os.path.join(output_folder, feature_id + '_' + huc + '_rating_curve_geo.csv'))


def manage_geo_rating_curves_production(ras2fim_output_dir, version, job_number, output_folder, overwrite):
    """
    This function sets up the multiprocessed generation of geo version of feature_id-specific rating curves.
    
    Args:
        ras2fim_output_dir (str): Path to top-level directory storing RAS2FIM outputs for a given run.
        version (str): Version number for RAS2FIM version that produced outputs.
        job_number (int): The number of jobs to use when parallel processing feature_ids.
        output_folder (str): The path to the output folder where geo rating curves will be written.
    """
    
    overall_start_time = datetime.now()
    dt_string = datetime.now().strftime("%m/%d/%Y %H:%M:%S")
    print (f"Started: {dt_string}")
        
    # Check job numbers and raise error if necessary
    total_cpus_available = os.cpu_count() - 1
    if job_number > total_cpus_available:
        raise ValueError('The job number, {}, '\
                          'exceeds your machine\'s available CPU count minus one ({}). '\
                          'Please lower the job_number.'.format(job_number, total_cpus_available))
    
    # Set up output folders.
    if not os.path.exists(ras2fim_output_dir):
        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), ras2fim_output_dir)
    if not os.path.exists(output_folder):
        os.mkdir(output_folder)
    else:
        if not overwrite:
            print("The output directory, " + output_folder + ", already exists. Use the overwrite flag (-o) to overwrite.")
            quit()
        else:
            shutil.rmtree(output_folder)
            os.mkdir(output_folder)
        
    # Define path to step 5 outputs
    hec_ras_output_path = os.path.join(ras2fim_output_dir, '05_hecras_output')  
    if not os.path.exists(hec_ras_output_path):
        print("No hec_ras output found for step 5. Expected folder path: " + hec_ras_output_path)
        quit()
    
    # Set up multiprocessing
    dictionary = {}
    local_dir_list = os.listdir(hec_ras_output_path)
    for huc in local_dir_list:
        full_huc_path = os.path.join(hec_ras_output_path, huc)
        if not os.path.isdir(full_huc_path):
            continue
        feature_id_list = os.listdir(full_huc_path)
        for feature_id in feature_id_list:
            full_feature_id_path = os.path.join(full_huc_path, feature_id)
            depth_grid_dir = os.path.join(ras2fim_output_dir, '06_metric', 'Depth_Grid', huc, feature_id)
            rating_curve_path = os.path.join(full_feature_id_path, 'Rating_Curve', feature_id + '_rating_curve.csv')
            try:
                depth_grid_list = os.listdir(depth_grid_dir)
            except FileNotFoundError:
                print(depth_grid_dir)
                continue
            full_path_depth_grid_list = []
            for depth_grid in depth_grid_list:
                full_path_depth_grid_list.append(os.path.join(depth_grid_dir, depth_grid))
            dictionary.update({feature_id: {'huc': huc, 'rating_curve': rating_curve_path, 'depth_grids': full_path_depth_grid_list}})
        
    print("Multiprocessing " + str(len(dictionary)) + " feature_ids using " + str(job_number) + " jobs...")
    with ProcessPoolExecutor(max_workers=job_number) as executor:
        for feature_id in dictionary:
            executor.submit(produce_geocurves, feature_id, dictionary[feature_id]['huc'], 
                            dictionary[feature_id]['rating_curve'], dictionary[feature_id]['depth_grids'], 
                            version, output_folder)
            
#            produce_geocurves(feature_id, dictionary[feature_id]['huc'], dictionary[feature_id]['rating_curve'], 
#                                      dictionary[feature_id]['depth_grids'], version, output_folder)
            
    # Calculate duration
    end_time = datetime.now()
    dt_string = datetime.now().strftime("%m/%d/%Y %H:%M:%S")
    print (f"Ended: {dt_string}")
    time_duration = end_time - overall_start_time
    print(f"Duration: {str(time_duration).split('.')[0]}")
    print()


if __name__ == '__main__':
    
    # Parse arguments
    parser = argparse.ArgumentParser(description = 'Produce Geo Rating Curves for RAS2FIM')
    parser.add_argument('-f', '--ras2fim_output_dir', help='Path to directory containing RAS2FIM outputs',
                        required=True)
    parser.add_argument('-v', '--version', help='RAS2FIM Version number',
                        required=True)
    parser.add_argument('-j','--job_number',help='Number of processes to use', required=False, default=1, type=int)
    parser.add_argument('-t', '--output_folder', help = 'Target: Where the output folder will be', required = False)
    parser.add_argument('-o','--overwrite', help='Overwrite files', required=False, action="store_true")
    
    #TODO Add -mc flag and use ras2catchments logic
    
    args = vars(parser.parse_args())
    manage_geo_rating_curves_production(**args)